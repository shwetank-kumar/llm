{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running models with and without quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "## GPT-2 has 1.5B parameters. 7B parameter models don't fit well on this GPU without quantization\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "memory_used = torch.cuda.memory_allocated()/1024./1024./1024.\n",
    "print(\"Memory used (GB): \", round(memory_used,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt and output\n",
    "prompt = \"India has diverse religions and culture\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**input, max_new_tokens=50)\n",
    "output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not great output - repetitive. Small model and not much was done to optimize the decoding - see strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 16 bit quantized 7B model - Zephyr Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restart the notebook to clear GPU memory housing previous model's weights. Using 8 bit quantization.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "memory_used = torch.cuda.memory_allocated()/1024./1024./1024.\n",
    "print(\"Memory used (GB): \", round(memory_used,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"India has diverse religions and culture\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**input, max_new_tokens=50)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better and more coherent response with bigger model but with 16 bit quantized version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 8 bit quantized 7B model - Zephyr Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restart the notebook to clear GPU memory housing previous model's weights. Using 8 bit quantization.\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", load_in_8bit=True)\n",
    "memory_used = torch.cuda.memory_allocated()/1024./1024./1024.\n",
    "print(\"Memory used (GB): \", round(memory_used,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"India has diverse religions and culture\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**input, max_new_tokens=50)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better and more coherent response with bigger model but with 8 bit quantized version. Still repetitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 4 bit quantized 7B model - Zephyr beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the notebook to clear GPU memory housing previous model's weights. Using 8 bit quantization.\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", load_in_4bit=True)\n",
    "memory_used = torch.cuda.memory_allocated()/1024./1024./1024.\n",
    "print(\"Memory used (GB): \", round(memory_used,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"India has diverse religions and culture\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**input, max_new_tokens=50)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better and more coherent response with bigger model but with 4 bit quantized version. Still repetitive and looks better than 8 bit version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different decoding strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets start with the biggest model that will fit in the memory footprint\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "prompt = \"India has diverse religions and culture\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy\n",
    "import json\n",
    "generated_ids = model.generate(**input, max_new_tokens=500)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output_dict = {\"method\": \"Greedy\", \"output\": output}\n",
    "with open('greedy.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beam\n",
    "import json\n",
    "generated_ids = model.generate(**input, num_beams=5, max_new_tokens=500)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output_dict = {\"method\": \"Beam\", \"output\": output}\n",
    "with open('beam.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam with multinomial sampling\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(0)\n",
    "generated_ids = model.generate(**input, num_beams=5, do_sample=True, max_new_tokens=500)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output_dict = {\"method\": \"Multinomai Beam\", \"output\": output}\n",
    "with open('multinomial_beam.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse beam search decoding\n",
    "from transformers import set_seed\n",
    "\n",
    "generated_ids = model.generate(**input, num_beams=5, num_beam_groups=5, diversity_penalty=1.0, max_new_tokens=500)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output_dict = {\"method\": \"Diverse Beam\", \"output\": output}\n",
    "with open('diverse_beam.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top p\n",
    "from transformers import set_seed\n",
    "\n",
    "generated_ids = model.generate(**input,  do_sample=True, top_p=0.95, top_k=0, temperature=0.6, max_new_tokens=500)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output_dict = {\"method\": \"Top p\", \"output\": output}\n",
    "with open('top_p.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive search\n",
    "from transformers import set_seed\n",
    "\n",
    "generated_ids = model.generate(**input, penalty_alpha=0.6, top_k=4, max_new_tokens=500)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "output_dict = {\"method\": \"Contrastive Search\", \"output\": output}\n",
    "with open('contrastive_search.json', 'w') as json_file:\n",
    "    json.dump(output_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assisted decoding for fast response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and assistant model should have the same tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "prompt = \"India has diverse religions and culture\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True,top_k=5, temperature=0.6, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token similarity matrix to benchmark text quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Benchmark\n",
    "from utils import plot_self_similarity\n",
    "import json\n",
    "\n",
    "benchmark = Benchmark(\"HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./greedy.json\", 'r') as json_file:\n",
    "        textdata = json.load(json_file)[\"output\"]\n",
    "\n",
    "ss = benchmark.analyze(textdata)\n",
    "plot_self_similarity(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./contrastive_search.json\", 'r') as json_file:\n",
    "        textdata = json.load(json_file)[\"output\"]\n",
    "\n",
    "ss = benchmark.analyze(textdata)\n",
    "plot_self_similarity(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
